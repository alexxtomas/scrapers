# Scraper

## IntroducciÃ³n

---

El web scraping (o raspado web) es una tÃ©cnica utilizada para extraer informaciÃ³n de sitios web. Consiste en utilizar programas o scripts para automatizar la navegaciÃ³n por un sitio web y recolectar informaciÃ³n de manera automatizada. Los datos recolectados pueden ser utilizados para diversos fines, como la monitorizaciÃ³n de precios, la recopilaciÃ³n de datos para anÃ¡lisis, la generaciÃ³n de contenido automatizado, entre otros. Algunos ejemplos de uso del web scraping incluyen el anÃ¡lisis de tendencias en redes sociales, la extracciÃ³n de datos de sitios de comparaciÃ³n de precios, la recopilaciÃ³n de noticias y actualizaciones de precios en sitios de comercio electrÃ³nico.

## Como se hace el web scraping?

---

La manera de hacer web scraping puede variar, dependiendo de si la pÃ¡gina se renderiza en el cliente es decir en nuestro navegador o si por lo contrario esta se renderiza en el servidor.

No hace falta que entiendas estos conceptos ahora mismo, solo tienes que quedarte con un par de detalles.

### Caso 1 - El cÃ³digo ya esta cargado

---

Si entramos a [Amazon](https://www.amazon.es/ref=nav_logo) e inspeccionamos el cÃ³digo fuente de la pÃ¡gina podremos ver como todo el HTML ya esta disponible. Podemos acceder al cÃ³digo fuente pulsando `CTRL+ U` o click derecho en la pÃ¡gina y pulsando sobre inspeccionar.

![Screenshot_20230115_090459.png](public/Screenshot_20230115_090459.png)

Como verÃ¡s nos abre una pestaÃ±a con mucho cÃ³digo HTML este es el que se representa en la pÃ¡gina y ya lo tenemos disponible desde un principio, esto significa que no estamos cargando nosotros el cÃ³digo si no que viene desde el servidor ya cargado para que nosotros lo podamos consumir, esto harÃ¡ que nuestro scraping sea mas eficiente y requiera de menos recursos. Para este caso vamos a utilizar [Cheerio](https://cheerio.js.org/index.html).

### Caso 2 - El cÃ³digo aun no esta cargado

---

A diferencia de Amazon hay otras pÃ¡ginas la cuÃ¡l no nos responden con el cÃ³digo ya cargado y es el cliente es decir nosotros somos los que el que tenemos que cargar este cÃ³digo.

Por ejemplo si nos dirigimos a [Youtube](https://www.youtube.com/) y hacemos lo mismo que en el caso anterior, podremos ver que el HTML que vemos en la pÃ¡gina no esta disponible al inspeccionar el cÃ³digo fuente.

Esto es debido a que es una pÃ¡gina dÃ­namica.

En este caso para hacer scraping en una pÃ¡gina como esta necesitamos de una herramienta que nos permita levantar un navegador para ir seleccionando los elementos de la pÃ¡gina.

Para ello existen varias herramientas que permiten hacerlo, en esta guÃ­a vamos a utilizar [Pupeteer](https://pptr.dev/) que es una de la mÃ¡s utilizadas.

Esta manera de hacer scraping es la menos eficiente ya que cada vez que tenemos que realizar scraping necesitamos levantar un navegador entero y esto es bastante costoso.

Pero por otro lado esta muy guay ya que es muy visual por que podemos visualizar como el navegador va ejecutando las instrucciones que recibe.

## Antes de empezarâ€¦

---

Si te es difÃ­cil seguir el paso a paso o algo no te funciona correctamente tienes el cÃ³digo ya resuelto y totalmente funcional en este repositorio.

[https://github.com/alexxtomas/scrapers](https://github.com/alexxtomas/scrapers)

Contiene tanto el scraper de Amazon como el de Youtube.

## Realizando scraping a Amazon

---

Como hemos visto anteriormente para realizar scraping Amazon necesitamos aplicar el caso 1 es decir utilizar la herramienta [Cheerio](https://cheerio.js.org/index.html).

Bien, pero que es realmente Cheerio?

Cheerio es una librerÃ­a open-source que nos ayuda a extraerÃ¡ informaciÃ³n de un cÃ³digo HTML que le proporcionemos, proveyÃ©ndonos una API para manipular los datos.

Empecemos a crear el proyecto.

1. Creamos la carpeta que contendrÃ¡ nuestro scraper y entramos en ella.

   ```bash
   mkdir amazon_scraper

   cd amazon_scraper
   ```

2. Inicializamos un proyecto de node.

   ```bash
   npm init -y
   ```

3. AÃ±adimos type module para utilizar ECMAScript modules en el package.json

   ```json
   "type": "module"
   ```

4. Instalamos las dependencias que vamos a necesitar.

   ```bash
   npm install cheerio mongoose
   ```

   Como dependencias de desarrollo instalaremos un linter para que nuestro cÃ³digo sea lo mÃ¡s solido posible.

   ```bash
   npm install -D standard
   ```

5. Configuramos el linter para ello aÃ±adimos las siguientes lÃ­neas al final de nuestro `package.json`

   ```json
   // ./amazon_scraper/package.json
   // ...
   "eslintConfig": {
       "extends": "./node_modules/standard/eslintrc.json",
       "rules": {
         "space-before-function-paren": 0
       }
     }
   ```

6. Creamos el punto de entrada de nuestro scraper.

   ```bash
   touch index.js
   ```

Una vez seguidos estos pasos debes tener una carpeta amazon_scraper que cuenta con un proyecto de node con sus respectivas dependencias y un archivo `index.js` vacÃ­o.

Empecemos a crear nuestro scraper de Amazon.

1. Creamos un archivo con el nombre `scraper.js`

   ```bash
   touch amazon_scraper.js
   ```

2. Importamos la funciÃ³n `load` desde `cheerio` la cual recibirÃ¡ como parÃ¡metro el cÃ³digo HTML de la pÃ¡gina y nos permitirÃ¡ interactuar sobre Ã©l.

   ```jsx
   // ./amazon_scraper/scraper.js
   import { load } from 'cheerio'
   ```

3. Ahora vamos a crear una funciÃ³n asÃ­ncrona con el nombre de `scrapeProducts` la cuÃ¡l recibirÃ¡ la URL de los productos de Amazon a scrapear y harÃ¡ la magia.

   Vayamos paso a paso.

   1. Realizamos una peticiÃ³n GET mediante `fetch` para recibir el HTML de Amazon. Todo el cÃ³digo que escribamos en la funciÃ³n ira dentro un `try catch` . TambiÃ©n al iniciar la funciÃ³n y durante todo el scraper iremos dejando `console.log` para informar de los procesos o errores de este mismo.

      ```jsx
      // ./amazon_scraper/scraper.js
      // ...
      export async function scrapeProducts(URL) {
      	console.log('Fetching Products...ðŸ¤–')
      	try {
          const response = await fetch(URL)
          const html = await response.text()

      	} catch(err) {
      			console.error('Something went wrong when scraping the data âŒ')
      	    throw err
      }
      ```

      P.D: Si Amazon fuese una pÃ¡gina que no contiene el HTML cuando inspeccionamos la pÃ¡gina como hemos visto en el caso 2 esta respuesta no contendrÃ­a el HTML que necesitamos de la pÃ¡gina.

   2. Si todo ha ido bien en nuestra variable `html` tenemos el cÃ³digo que necesitamos. Si eres un poco avispado sabras que debemos hacer ahora, si tenemos que pasar este cÃ³digo HTML a la funciÃ³n `load` que hemos importado anteriormente de `cheeerio`, para que podamos empezar extraer la informaciÃ³n que necesitamos.

      ```jsx
      // ./amazon_scraper/scraper.js
      // ...
      export async function scrapeProducts(URL) {
      	console.log('Fetching Products...ðŸ¤–')
      	try {
          // ...
      		const $ = load(html)

      	} catch(err) {
      			console.error('Something went wrong when scraping the data âŒ')
      	    throw err
      }

      ```

   3. Bien, ya podemos interactuar con este cÃ³digo html gracias a la API que nos proporciona Cheerio. Para conocer cuales son todas las opciones que este nos proporciona te recomiendo que leas su documentaciÃ³n. Pero a groso modo este nos permitirÃ¡ encontrar elementos en el cÃ³digo mediante selectores de `CSS` . AsÃ­ que nuestra tarea se basa en encontrar atributos de la etiqueta html que sean Ãºnicos para la informaciÃ³n que queremos encontrar. En este caso queremos los productos de una bÃºsqueda de Amazon, veamos que atributos podemos extraer.

      ![Screenshot_20230116_031454.png](public/Screenshot_20230116_031454.png)

      Como puedes ver el elemento padre, es decir el elemento que contiene toda la informaciÃ³n sobre el producto es un `div` con las siguiente clase `sg-col-4-of-24 sg-col-4-of-12 s-result-item s-asin sg-col-4-of-16 AdHolder sg-col s-widget-spacing-small sg-col-4-of-20` . Si capturamos todos estos divs (los padres) por cada producto, posteriormente podremos buscar dentro de el (sus hijos) toda la informaciÃ³n del producto.

   4. Extraemos la informaciÃ³n que necesitamos y la guardamos en una variable llamada `products`.

      ```jsx
      // ./amazon_scraper/scraper.js
      // ...
      export async function scrapeProducts(URL) {
      	console.log('Fetching Products...ðŸ¤–')
      	try {
          // ...
      		// Creamos un array donde almacenaremos nuestros productos
      		const products = []
      		// Selecionamos mediante un selector de css el elemento padre de cada producto
          $(
            'div.sg-col-4-of-24.sg-col-4-of-12.s-result-item.s-asin.sg-col-4-of-16.AdHolder.sg-col.s-widget-spacing-small.sg-col-4-of-20'
          ).each((_i, el) => {
      			// _i es igual al indice, el es igual al elemento html
      			// Cada elemento es igual al div de cada producto que hemos visto anteriormente
            const product = $(el)
      			/* Buscamos la informaciÃ³n mediante selectores de CSS y mediante .text() o .attr()
      				 ndicamos la informaciÃ³n con la que nos queremos quedar*/
            const title = product.find('span.a-size-base-plus.a-color-base.a-text-normal').text()
            const image = product.find('img.s-image').attr('src')

            const link = product.find('a.a-link-normal.a-text-normal').attr('href')

            const reviews = product
              .find('div.a-section.a-spacing-none.a-spacing-top-micro > div.a-row.a-size-small')
              .children('span')
              .last()
              .attr('aria-label')

            const stars = product
              .find('div.a-section.a-spacing-none.a-spacing-top-micro > div > span')
              .attr('aria-label')

            const price = product.find('span.a-price > span.a-offscreen').text()

      			// Creamos nuestro objeto con todos los elementos del producto
            const element = {
              title,
              image,
              link: `https://amazon.com${link}`,
              price
            }
      			/* Debido a que no todos los productos tienen reviews o estrellas
      				 los aÃ±adimos al producto solo si existen*/
            if (reviews) {
              element.reviews = Number(reviews)
            }

            if (stars) {
              element.stars = stars
            }
      			// AÃ±adimos nuestro producto a nuestro array de productos
            products.push(element)
          })
      		// Devolvemos nuestro array de productos
          return products
      	} catch(err) {
      			console.error('Something went wrong when scraping the data âŒ')
      	    throw err
      }

      ```

   Con este cÃ³digo estamos extrayendo la informaciÃ³n por cada producto. Es normal que no lo entiendas todo a la perfecciÃ³n ya que para ello debes leer la documentaciÃ³n de `Cheerio` . Simplemente quÃ©date con que como hemos visto anteriormente los productos contienen un elemento div que consta con una clase la cuÃ¡l utilizamos para seleccionarlo dentro del cÃ³digo HTML y posteriormente buscamos la informaciÃ³n del producto dentro de este mismo.

   Por ejemplo donde estamos extrayendo la imagen

   ```jsx
   const image = product.find('img.s-image').attr('src')
   ```

   Debes leerlo de esta manera, dentro de cada div de los productos bÃºscame una elemento `img` con la clase `s-image` y de este elemento dame el contenido de su `src`

Perfecto ya contamos con nuestra funciÃ³n la cuÃ¡l scrapeara la informaciÃ³n de los productos, pero tambiÃ©n queremos almacenar la informaciÃ³n en una base de datos, en este caso [MongoDB](https://www.mongodb.com/) , para ello hemos instalado [Mongoose](https://mongoosejs.com/docs/guide.html).

Empecemos a crear nuestro modelo y conexiÃ³n a la base de datos.

1. Creamos el archivo el cuÃ¡l contendrÃ¡ la funciÃ³n que nos permitirÃ¡ conectarnos a la base de datos.

   ```bash
   touch db_connection.js
   ```

2. Creamos la funciÃ³n para conectarnos a la base de datos.

   ```jsx
   // ./amazon_scraper/db_connection.js
   import mongoose from 'mongoose'

   // Nos conectamos a la base de datos
   export async function connectToDb(URI) {
     console.log('Connecting to database...ðŸ¤–')
     await mongoose
       .set('strictQuery', false)
       .connect(URI)
       .then((db) => console.log(`Connected to database -> ${db.connection.name} âœ…`))
       .catch((err) => {
         console.error('Failed to connect to database âŒ')
         throw err
       })
   }
   ```

   La URI vendrÃ¡ por parÃ¡metro por que dependiendo de los productos que queramos scrapear el nombre de la colecciÃ³n de `MongoDB` cambiara.

3. Creamos el archivo que contendrÃ¡ nuestro modelo de `Mongoose` para los productos scrapeados.

   ```jsx
   touch product_model.js
   ```

4. Creamos el modelo

   ```jsx
   import { model, Schema } from 'mongoose'

   const productSchema = new Schema(
     {
       title: { type: String, required: true, unique: true },
       image: {
         type: String,
         required: true,
         // AÃ±admios una imagen por defecto por si acaso.
         default: 'https://www.odoo.com/web/image/res.users/1072846/image_1024?unique=3f33558'
       },
       link: { type: String, required: true, unique: true },
       price: { type: String, required: true },
       /* Como hemos visto en el scraper estos valores pueden no estar presentes 
   			 por eso no son requeridos. */
       reviews: Number,
       stars: String
     },
     {
       // MostrarÃ¡ la fecha de craciÃ³n o Ãºltima modificaciÃ³n
       timestamps: true,
       // Mongoose elimina el __v que crea MongoDB cuando nos devuelve la informaciÃ³n
       versionKey: false
     }
   )

   export const Product = model('Product', productSchema)
   ```

El scraper va a recibir por argumentos el producto a buscar en Amazon, para acceder a los argumentos que se le pasan por terminal en node podemos utilizar el [process.argv](https://www.geeksforgeeks.org/node-js-process-argv-property/) . Simplemente vamos a acceder a estos argumentos y comprobar que se estÃ¡n introduciendo correctamente en nuestro `index.js` , tambiÃ©n vamos a importar todo lo que necesitaremos en este archivo y ejecutar las funciones.

1. Importamos las funciones creadas anteriormente y nuestro modelo.

   ```jsx
   // ./amazon_scraper/index.js
   import { connectToDb } from './db_connection.js'
   import { Product } from './product_model.js'
   import { scrapeProducts } from './scraper.js'
   ```

2. Accedemos a los valores de `process.argv` y realizamos distintas comprobaciones para asegurarnos de que nuesto scraper funcione correctamente.

   ```jsx
   // ./amazon_scraper/index.js
   // ...
   const [, , ...productToSearch] = process.argv

   if (productToSearch.length > 1) {
     console.log(
       'If the product name contains spaces, enter the product name in quotes like this "star wars"ðŸ¤–'
     )
     process.exit(1)
   }

   if (!productToSearch.length) {
     console.log('Please provide a product to search!!!ðŸ¤–')
     process.exit(1)
   }
   ```

3. Formateamos los el valor introducido para que este sea valido.

   ```jsx
   // ./amazon_scraper.js
   // ...
   const productForURL = productToSearch[0].replaceAll(' ', '+')
   const productForURI = productToSearch[0].replaceAll(' ', '_')
   ```

   Para la URL necesitamos remplazar los espacios por `+` debido a que en la URL de amazon los espacios de remplazan por este signo. Prueba a buscar en Amazon star wars veras como en la URL star wars de cambia a star+wars.

   Para la URI necesitamos remplazar los espacios por cualquier otro valor ya que romeria la URI el tener espacios. Este valor lo asignaremos a la URI para que el nombre de la colecciÃ³n de MongoDB se asocie al nombre de la bÃºsqueda realizada.

4. AÃ±adimos los valores a la URI y URL .

   ```jsx
   // ./amazon_scraper.js
   // ...
   const URL = `https://www.amazon.es/s?k=${productForURL}&crid=2M8FD9OZE91ZJ&sprefix=%2Caps%2C92&ref=nb_sb_ss_recent_1_0_recent`
   // Debes cambiar la URI por la tuya y aÃ±adir Amazon_${productForURI} como esta aÃ±adido en este ejemplo
   const URI = `mongodb+srv://root:root@cluster0.fvg4s0k.mongodb.net/Amazon_${productForURI}?retryWrites=true&w=majority`
   ```

5. Nos conectamos a la base de datos.

   ```jsx
   // ./amazon_scraper.js
   // ...
   await connectToDb(URI)
   ```

6. Empezamos a scrapear los productos de la URL y los guardamos en la base de datos.

   ```jsx
   // ./amazon_scraper.js
   // ...
   scrapeProducts(URL).then(async (products) => {
     console.log(`Fetched ${products.length} products successfullyâœ…`)
     console.log('Saving to database...ðŸ¤–')
     await Product.insertMany(products)
       .then(() => console.log('Products saved succesfully to databaseâœ…'))
       .catch((err) => {
         console.error('Something went wrong when saving products to databaseâŒ')
         throw err
       })
   })
   ```

Ya tenemos nuestro scraper creado, pero como lo hacemos funcionar? Para ello vamos hacer que nuestro scraper actÃºe como un script.

1. AÃ±adimos esta lÃ­nea al principio de todo de nuestro `index.js`

   ```jsx
   #!/usr/bin/env node
   ```

2. AÃ±adimos nuestro `index.js` al apartado [bin](https://docs.npmjs.com/cli/v9/configuring-npm/package-json#bin) del `package.json`.

   ```jsx
   // ./amazon_scraper/package.json
   // ...
   "bin": {
       "amazonScraper": "./index.js"
     },
   ```

Con esto podemos ejecutar el comando `npm link` en una terminal dentro de la carpeta `amazon_scraper` y posteriormente ejecutar `amazonScraper nombreDeLaBusqueda` y se ejecutarÃ¡ nuestro cÃ³digo. Hay que tener en cuenta que si queremos buscar un valor que tenga espacios debemos aÃ±adirlo entre `â€œâ€` , por ejemplo `amazonScraper â€œstar warsâ€` .

## Realizando scraping a Youtube

---

En este caso aplicamos el caso 2 debido a que [Youtube](https://www.youtube.com/) es una pÃ¡gina dinÃ¡mica y no contamos con el HTML al inspeccionar la pÃ¡gina. Deberemos utilizar librerÃ­as como [Puppeteer](https://pptr.dev/).

QuÃ© es realmente Puppeteer ? Es una librerÃ­a que nos proporciona una API de alto nivel con la cuÃ¡l podemos controlar Chrome. De lo que trata este tipo de scraping es de levantar un navegador entero y controlarlo mediante la lÃ­breria. Deberemos selecionar mediante selectores de css los elementos con los que queremos interactuar y el navegador interactuarÃ¡ con ellos como si de un usuario se tratase.

Como en el anterior scraping que hemos hecho a Amazon necesitamos crear una carpeta y en ella inicializar un proyecto de Node. A esta carpeta yo le voy a llamar `youtube_scraper`.

Una vez hecho esto instalamos las dependencias que vamos a necesitar.

```bash
npm i mongoose puppeteer ytdl-core
```

[ytdl-core](https://github.com/fent/node-ytdl) es una libreria que nos permitirÃ¡ obtener informaciÃ³n de los videos de scrapemos para aÃ±adir esta informaciÃ³n a nuestra base de datos. Esta libreria tambiÃ©n nos permite descargar los videos asi que te puedes animar y crear un descargador de videos de Youtube.

Posteriormente aÃ±admis el linter.

```bash
npm i -D standard
```

```json
// ./youtube_scraper/package.json
// ...
"eslintConfig": {
    "extends": "./node_modules/standard/eslintrc.json",
    "rules": {
      "space-before-function-paren": 0
    }
  }

```

El funcionamiento de este scraper va a ser igual que el del anterior, ejecutaremos en terminal el nombre del script y le pasaremos la bÃºsqueda que queremos realizar. Para ello aÃ±adamos el `bin` en el `package.json` .

```json
// ./youtube_scraper/package.json
// ...
"bin": {
    "youtubeScraper": "./index.js"
  },
// ...
```

El cÃ³digo de nuestro scraper va a estar en el `index.js` dentro de una funciÃ³n autoejecutada. Este es el cÃ³digo que va a dar instrucciones al navegador, va a esperar a ciertos eventos y seleccionara elementos de Youtube mediante selectores de CSS. La otra parte del cÃ³digo como puede ser la conexiÃ³n a la base de datos entre otras funcionalidades estarÃ¡n separadas en una carpeta `logic` .

TambiÃ©n contaremos con una carpeta llamada `models` que contendra nuestros modelos de `Mongoose`.

### Creamos nuestros modelos

---

Creamos la carpeta models y aÃ±adimos dos archivos, uno llamado `author_model.js` y otro llamado `video_model.js`

```jsx
mkdir models
touch author_model.js
touch video_model.js
```

En el archivo author_model.js vamos a crear nuestro model tal que asÃ­

```jsx
// ./youtube_scraper/models/author_model.js
import { model, Schema } from 'mongoose'

const authorSchema = new Schema(
  {
    name: { type: String, required: true },
    user: { type: String, required: true },
    channelURL: { type: String, required: true },
    userURL: { type: String, required: true },
    verified: { type: Boolean, required: true },
    subscribers: { type: String, required: true },
    videos: [{ type: Schema.Types.ObjectId, ref: 'Video' }]
  },
  {
    timestamps: true,
    versionKey: false
  }
)

export const Author = model('Author', authorSchema)
```

Y el modelo Video lo vamos a crear asÃ­.

```jsx
// ./youtube_scraper/models/video_model.js
import { model, Schema } from 'mongoose'

const videoSchema = new Schema(
  {
    title: { type: String, required: true },
    description: { type: String },
    videoDuration: { type: String, required: true },
    isFamilySafe: { type: Boolean, required: true },
    viewCount: { type: String, required: true },
    category: { type: String, required: true },
    publishDate: { type: String, required: true },
    keywords: { type: [String], required: true },
    author: { type: Schema.Types.ObjectId, ref: 'Author' },
    isPrivate: { type: Boolean, required: true },
    isLiveContent: { type: Boolean, required: true },
    likes: { type: String, required: true },
    dislikes: { type: String, required: true },
    ageRestricted: { type: Boolean, required: false },
    link: { type: String, required: true }
  },
  {
    timestamps: true,
    versionKey: false
  }
)

export const Video = model('Video', videoSchema)
```

## Creamos la lÃ³gica de nuestra apliaciÃ³n

---

Vamos a crear la carpeta `logic` y dentro de ella una llamada `db` .

```bash

mkdir logic
cd logic
mkdir db
```

Dentro de `db` crearemos dos archivos, uno llamado `db_connection.js` y `save_data_to_db.js`

```bash
touch db_connection.js
touch save_data_to_db.js
```

En el archivo `db_connection.js` crearemos la funciÃ³n que nos permitira conectarnos a la base de datos.

```jsx
// ./youtube_scraper/logic/db/db_connection.js
import mongoose from 'mongoose'

export async function connectToDb(URI) {
  console.log('Connecting to database...ðŸ¤–')
  await mongoose
    .set('strictQuery', false)
    .connect(URI)
    .then(() => console.log('Connected to database âœ…'))
    .catch((err) => {
      console.error('Failed to connect to database âŒ')
      throw err
    })
}
```

En el archivo `save_data_to_db.js` creamos la funciÃ³n con la cuÃ¡l guardaremos en la base de datos los autores y los videos que hemos scrapeado.

```jsx
// ./youtube_scraper/logic/db/save_data_to_db.js
import { Author } from '../../models/author_model.js'
import { Video } from '../../models/video_model.js'
import { log } from '../log.js'
export async function saveDataToDb({ authors, videos }) {
  Promise.all([await Author.insertMany(authors), await Video.insertMany(videos)])
    .then(() => {
      log({
        message: 'All videos and their authors have been saved to the database successfully ðŸš€'
      })
    })
    .catch((err) => {
      log({ message: 'Error while saving data to database âŒ', err: true })
      throw err
    })
}
```

Como se puede ver se esta importando una funciÃ³n llamada `log` la cuÃ¡l se esta utilizando como un `console.log`, lo Ãºtil de esta funciÃ³n es que nos permite aÃ±adir a todos los console.log un prefijo. Vamos a crearla.

```jsx
// ./youtube_scraper/logic/log.js

const PREFIX = 'Youtube Scraper ðŸ¤– - '

export function log({ message, err = false }) {
  if (!err) {
    console.log(PREFIX, message)
  } else {
    console.error(PREFIX, message)
  }
}
```

Como hemos hecho anteriormente necesitamos extraer el valor que queremos buscar de la terminal, para ello creamos el archivo `extract_video_to_search.js` y en el crearemos una funciÃ³n la cuÃ¡l extraerÃ¡ este valor como hemos hecho en el scraper anterior.

```jsx
// ./youtube_scraper/logic/extract_video_to_search.js
export function extractVideoToSearch() {
  const [, , ...valueProvided] = process.argv

  if (valueProvided.length > 1) {
    console.log(
      'If the product name contains spaces, enter the product name in quotes like this "star wars"ðŸ¤–'
    )
    process.exit(1)
  }

  if (!valueProvided.length) {
    console.log('Please provide a product to search!!!ðŸ¤–')
    process.exit(1)
  }
  const [videoToSearch] = valueProvided

  return { videoToSearch, formattedVideoToSearch: videoToSearch.replaceAll(' ', '_') }
}
```

Por Ãºltimo tenemos que crear una funciÃ³n para formatear los datos. La librerÃ­a `ytdl-core` nos proporciona una funciÃ³n la cuÃ¡l nos permite obtener informaciÃ³n de un video facilitÃ¡ndole un link.

Nosotros no queremos toda la informaciÃ³n ya que hay mucha que no nos sirve para nada. Vamos a extraer la informaciÃ³n de lo que nos devuelve esta funciÃ³n a lo que nosotros queremos. Vamos a querer quedarnos con la informaciÃ³n que previamente hemos definido en los modelos. En esto consiste esta funciÃ³n en quedarnos solo con la informaciÃ³n que nos interesa y ignorar la que no.

```jsx
// ./youtube_scraper/logic/format_data.js
import ytdl from 'ytdl-core'
export async function formatData({ authors, videos }) {
  for (const i in videos) {
    const video = videos[i]
    const { link } = video
    try {
      const { videoDetails } = await ytdl.getInfo(link)

      const {
        name,
        user,
        channel_url: channelURL,
        user_url: userURL,
        verified,
        subscriber_count: subscribers
      } = videoDetails.author
      authors[i] = {
        name,
        user,
        channelURL,
        userURL,
        verified,
        subscribers: subscribers.toString()
      }
      const {
        description,
        lengthSeconds: videoDuration,
        isFamilySafe,
        viewCount,
        category,
        publishDate,
        keywords,
        isPrivate,
        isLiveContent,
        likes,
        dislikes,
        age_restricted: ageRestricted,
        video_url: videoURL
      } = videoDetails
      videos[i] = {
        ...video,
        description: description ?? '',
        videoDuration: `${videoDuration} seconds`,
        isFamilySafe,
        viewCount,
        category,
        publishDate,
        keywords: keywords ?? [],
        isPrivate,
        isLiveContent,
        likes: likes ?? 'Unknown',
        dislikes: dislikes ?? 'Unknown',
        ageRestricted,
        videoURL
      }
    } catch (err) {
      console.error(`Error when formatting ${video.title} âŒ`)
    }
  }
}
```

### Creamos nuestro scraper

---

Bien toca crear ya la funcionalidad que como en el caso del scraper de Amazon nos permita obtener los datos de la pÃ¡gina. Solo vamos a obtener el titulo y el link de todos los videos que aparezcan en resultados cuando realicemos la busqueda mediante este metodo. Los demÃ¡s datos los vamos a obtener con la funciÃ³n creada anteriormete la cuÃ¡l se encuentra en el archivo `format_data.js` , en esta le pasamos el link de los videos que hemos scrapeado y esta obtiene y formatea todos los datos de los videos.

Vamos a scrapear los datos para ello en el `index.js` creamos una funciÃ³n autoejecutada, en la cuÃ¡l ejecutaremos toda la lÃ³gica de nuestro scraping e iremos ejecutando las funciones previamente creadas.

```jsx
#!/usr/bin/env node
// ./youtube_scraper/index.js
/* eslint-disable camelcase */
import puppeteer from 'puppeteer'
import { connectToDb } from './logic/db/db_connection.js'
import { saveDataToDb } from './logic/db/save_data_to_db.js'
import { extractVideoToSearch } from './logic/extract_video_to_search.js'
import { formatData } from './logic/format_data.js'
import { log } from './logic/log.js'
;(async () => {
  const { videoToSearch, formattedVideoToSearch } = extractVideoToSearch()

  // Acuerdate de pegar tu propia URI y aÃ±adir el valor de formattedVideoToSearch como en este ejemplo
  const URI = `mongodb+srv://root:root@cluster0.xmqbgxh.mongodb.net/${formattedVideoToSearch}?retryWrites=true&w=majority`

  log({ message: 'Configuring the browser' })

  // Configuramos el navegador que se va a lanzar para poder visualizarlo
  const browser = await puppeteer.launch({
    headless: false,
    defaultViewport: null,
    args: ['--start-maximized']
  })

  // Abrir una nueva pestaÃ±a
  log({ message: 'Opening the browser' })
  const page = await browser.newPage()

  log({ message: 'Going to youtube' })
  // Ir a la URL
  await page.goto('https://www.youtube.com')

  log({ message: 'Accepting youtube cookies' })
  // Esperar a que el botÃ³n este disponible
  const acceptAllButton = await page.waitForSelector(
    'button[aria-label="Accept the use of cookies and other data for the purposes described"]'
  )
  // Hacer click en el botÃ³n
  await acceptAllButton.click()

  // Esperar a que la pÃ¡gina se refresque
  await page.waitForNavigation()
  log({ message: 'Typing in the serach bar' })
  const searchInput = await page.$('input#search')
  // Escribir en la barra de bÃºsqueda
  await searchInput.type(videoToSearch)

  log({ message: 'Clicking on the search button' })
  // Esperar a que la el input contenga el valor que hemos introudcido
  await page.waitForFunction(`document.querySelector('input#search').value === '${videoToSearch}'`)
  // Hacer click en el botÃ³n para buscar
  await page.click('button#search-icon-legacy')

  await page.waitForNavigation()
  log({ message: 'Selecting the videos' })
  const videos = await page.evaluate(() => {
    // Obtener todos los titulos y links de los videos
    const videoElements = document.querySelectorAll('.ytd-video-renderer #video-title')
    const videos = []
    videoElements.forEach(({ textContent, href: link }) => {
      const title = textContent.replace(/\n/g, '').trim()
      videos.push({ title, link })
    })
    return videos
  })

  const totalVideos = videos.length
  log({ message: `${totalVideos} selected` })
  if (!totalVideos) {
    log({ message: 'Something went wrong, please try again âŒ', err: true })
    process.exit(1)
  }
  log({ message: 'Formatting all the videos. This may take a while, please wait.' })

  const authors = []

  await formatData({ authors, videos })
  log({ message: 'Videos formatted successfully!' })
  log({ message: 'Connecting to database...' })

  await connectToDb(URI)

  log({ message: 'Saving videos to database!. This may taye a while, please wait.' })
  await saveDataToDb({ authors, videos })
  log({ message: 'Finsihed the scraping' })
  setTimeout(() => {
    process.exit(1)
  }, 3000)
})()
```

Y listo para usarlo podemos debemos ejectuar dentro de nuestra carpeta `youtube_scraper` el comando `npm link` y a partir de ahora ejecutando en nuestra terminal `youtubeScraper` podremos scrapear los datos que queramos. Por ejemplo para scrapear videos sobre star wars realizaremos lo siguiente.

```bash
youtubeScraper "star wars"
```
